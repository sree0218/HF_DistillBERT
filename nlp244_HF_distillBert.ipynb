{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLK3w46PLdUR",
        "outputId": "b0e1194d-dd98-4d52-9d4c-adef249a143d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install transformers torch\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "3mWdbvzd_kKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/drive/MyDrive/NLP244/QUEST4/hf_snli/hf_libraries_demo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KYWDCVgTEUu",
        "outputId": "498450f5-eada-4605-c853-7218075b15f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP244/QUEST4/hf_snli/hf_libraries_demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "8jpB1jUfVMc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "DJJyHzkCWdkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets, load_dataset, DatasetDict\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Union, Callable, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import torch"
      ],
      "metadata": {
        "id": "8LkvZEj2_gzU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# loading the dataset will download if not present in your cache, or simply load from there\n",
        "filepath_train=\"/content/drive/MyDrive/snli/train.csv\"\n",
        "filepath_val=\"/content/drive/MyDrive/snli/val.csv\"\n",
        "filepath_test=\"/content/drive/MyDrive/snli/test.csv\"   \n",
        "train_ds = pd.read_csv(filepath_train)\n",
        "val_ds = pd.read_csv(filepath_val)\n",
        "test_ds = pd.read_csv(filepath_test)\n",
        "train_ds = train_ds.dropna()\n",
        "val_ds = val_ds.dropna()\n",
        "test_ds = test_ds.dropna()    \n",
        "train_dataset = Dataset.from_pandas(train_ds)\n",
        "val_dataset = Dataset.from_pandas(val_ds)\n",
        "test_dataset = Dataset.from_pandas(test_ds)\n",
        "french_snli_DD = datasets.DatasetDict({\"train\":train_dataset,\"validation\": val_dataset, \"test\":test_dataset})\n",
        "dataset: DatasetDict = french_snli_DD\n",
        "\n",
        "assert sorted(list(dataset.keys())) == ['test', 'train', 'validation'], f\"unexpected splits or keys! {dataset}\"\n",
        "print(dataset)\n",
        "pprint(train_dataset[0])\n",
        "for item in train_dataset:\n",
        "    assert 'hypothesis' in item and type(item['hypothesis']) == str\n",
        "    assert 'premise' in item and type(item['premise']) == str\n",
        "    assert 'label' in item and type(item['label']) == int\n",
        "\n",
        "# loading separately is the same as just accessing from DatasetDict:\n",
        "for item_a, item_b in zip(train_dataset, dataset['train']):\n",
        "    assert item_a == item_b, \"datasets aren't the same!\"\n"
      ],
      "metadata": {
        "id": "HV3MyKvHfXb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Dict, Callable\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from datasets import DatasetDict, load_dataset\n",
        "\n",
        "DataPoint = Dict[str, Any]\n",
        "\n",
        "\n",
        "def lowercase_text(item: DataPoint) -> DataPoint:\n",
        "    return {\"hypothesis\": item['hypothesis'].lower(), \"premise\": item['premise'].lower() }\n",
        "\n",
        "def pre_process_dataset(dataset: DatasetDict) -> DatasetDict:\n",
        "    dataset = dataset.map(lowercase_text)\n",
        "    assert 'label' in dataset['train'][0]  # note: non-destructive operation\n",
        "\n",
        "    dataset = dataset.map(\n",
        "        lambda item: {'hypothesis': ' '.join(item['hypothesis'].split())},\n",
        "        desc=\"normalizing all white space to a single space\"\n",
        "    )\n",
        "    dataset = dataset.map(\n",
        "        lambda item: {'premise': ' '.join(item['premise'].split())},\n",
        "        desc=\"normalizing all white space to a single space\"\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "    # see function above for details\n",
        "    dataset = pre_process_dataset(dataset)\n",
        "\n",
        "    # notice the reduced size due to filter\n",
        "    print(dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "LUznmbiYfXZU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "from transformers import TextClassificationPipeline, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers.pipelines.base import GenericTensor\n",
        "\n",
        "\n",
        "class PerfectTextClassificationPipeline(TextClassificationPipeline):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        model = AutoModel.from_pretrained(\"distilbert-base-cased\")\n",
        "        kwargs = {\n",
        "            \"framework\": \"pt\",\n",
        "            \"model\": model,\n",
        "            \"task\": \"text-classification\",\n",
        "            **kwargs\n",
        "        }\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def _sanitize_parameters(self, return_all_scores=None, function_to_apply=None, top_k=\"\", **tokenizer_kwargs):\n",
        "        return super()._sanitize_parameters(return_all_scores, function_to_apply, top_k, **tokenizer_kwargs)\n",
        "\n",
        "    def preprocess(self, inputs, **tokenizer_kwargs) -> Dict[str, GenericTensor]:\n",
        "        return inputs\n",
        "\n",
        "    def _forward(self, model_inputs):\n",
        "        # print(model_inputs)\n",
        "        if 'text_pair' not in model_inputs:\n",
        "            raise ValueError(\"this pipeline needs labels to cheat and get perfect performance! call compute with \"\n",
        "                             \"second_input_column='premise'\")\n",
        "\n",
        "        return {\"label\": model_inputs['label']}\n",
        "\n",
        "    def postprocess(self, model_outputs, function_to_apply=None, top_k=1, _legacy=True):\n",
        "        return model_outputs"
      ],
      "metadata": {
        "id": "sWHaPuG5g9to"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from evaluate import TextClassificationEvaluator, Metric, EvaluationModuleInfo\n",
        "from sklearn.metrics import f1_score\n",
        "class MyMacroF1Metric(Metric):\n",
        "    \"\"\"\n",
        "    You can define custom metrics! In this case I do this to compute Macro-F1, which averages per-class F1 scores\n",
        "    \"\"\"\n",
        "    f1_metric_info: EvaluationModuleInfo = evaluate.load(\"f1\")._info()\n",
        "\n",
        "    def _info(self) -> EvaluationModuleInfo:\n",
        "        return MyMacroF1Metric.f1_metric_info\n",
        "\n",
        "    def _compute(self, predictions=None, references=None, labels=None, pos_label=1, sample_weight=None) -> Dict[str, Any]:\n",
        "        score = f1_score(\n",
        "            references, predictions, labels=labels, pos_label=pos_label, average=\"macro\", sample_weight=sample_weight\n",
        "        )\n",
        "        return {\"f1\": float(score) if score.size == 1 else score}\n",
        "\n",
        "\n",
        "\n",
        "# lets set up a text classification evaluator\n",
        "text_eval: TextClassificationEvaluator = TextClassificationEvaluator('text-classification',\n",
        "                                                                      default_metric_name=\"accuracy\")\n",
        "\n",
        "# create a 'perfect' model and evaluate\n",
        "perfect_model: PerfectTextClassificationPipeline = PerfectTextClassificationPipeline()\n",
        "\n",
        "# you can also instantiate a metric yourself with evaluate.load:\n",
        "f1_metric: MyMacroF1Metric = MyMacroF1Metric()\n",
        "results = text_eval.compute(\n",
        "    model_or_pipeline=perfect_model,\n",
        "    data=dataset['test'],\n",
        "    input_column='hypothesis', \n",
        "    second_input_column='premise', \n",
        "    label_column='label',\n",
        "    metric=evaluate.combine(evaluations=[\"accuracy\", f1_metric]),\n",
        ")\n",
        "assert results['accuracy'] == results['f1'] == 1.0, \\\n",
        "    f\"we used the perfect pipeline, expected perfect prediction! got {results['accuracy']}\"\n",
        "print(\"==== Perfect Model Results (expected 1.0) ====\")\n",
        "pprint(results)"
      ],
      "metadata": {
        "id": "_kR64VbcVqFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict\n",
        "\n",
        "import datasets\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import torch.cuda\n",
        "import wandb as wandb\n",
        "from datasets import DatasetDict, load_dataset, Dataset\n",
        "from evaluate import EvaluationModule\n",
        "from torch import Tensor\n",
        "from transformers import AutoTokenizer, AutoModel,TrainingArguments, Trainer, EvalPrediction\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class LabelSmoothedTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    To add label-smoothing, we can just sub-class to compute loss with different parameters\n",
        "    \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        model_outputs = model(**inputs)\n",
        "        loss = F.cross_entropy(model_outputs['logits'], inputs['label'], label_smoothing=.01)\n",
        "        model_outputs['loss'] = loss\n",
        "        return (loss, model_outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
        "    model = AutoModel.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    train_dataset = train_dataset.map(lambda batch: tokenizer(batch['hypothesis'], batch['premise'],truncation=True), batched=True, batch_size=256)\n",
        "    val_dataset = val_dataset.map(lambda batch: tokenizer(batch['hypothesis'], batch['premise'],truncation=True), batched=True, batch_size=256)\n",
        "    test_dataset = test_dataset.map(lambda batch: tokenizer(batch['hypothesis'],batch['premise'], truncation=True), batched=True, batch_size=256)\n",
        "\n",
        "    # convert train set to tensors with only model inputs\n",
        "    train_dataset.set_format(type=\"pt\", columns=['input_ids', 'attention_mask', 'hypothesis', 'premise', 'label'])\n",
        "\n",
        "    f1_metric: MyMacroF1Metric = MyMacroF1Metric()\n",
        "    my_evaluation: EvaluationModule = evaluate.combine([\"accuracy\", f1_metric])\n",
        "\n",
        "    def my_compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
        "        logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
        "        predictions: Tensor = logits.argmax(axis=1)\n",
        "        return my_evaluation.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    # Let's fine-tune with the Trainer API!\n",
        "    training_args: TrainingArguments = TrainingArguments(\n",
        "        output_dir=\"/content/drive/MyDrive/NLP244/QUEST4/hf_snli/check_points\",\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        do_predict=True,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=128,\n",
        "        per_device_train_batch_size=64,\n",
        "        per_device_eval_batch_size=128,\n",
        "        save_steps=128,\n",
        "        save_strategy=\"steps\",\n",
        "        save_total_limit=5,\n",
        "        report_to=None,\n",
        "        logging_steps=50,\n",
        "        num_train_epochs=20,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        load_best_model_at_end=True,\n",
        "        dataloader_num_workers=2,  # set to 0 when debugging and >1 when running!\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer: LabelSmoothedTrainer = LabelSmoothedTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=None,  # let HF set this to an instance of transformers.DataCollatorWithPadding\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=my_compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model = trainer.model  # make sure to load_best_model_at_end=True!\n",
        "\n",
        "    # run a final evaluation on the test set\n",
        "    trainer.evaluate(metric_key_prefix=\"test\", eval_dataset=test_dataset)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fie6VJy3fXW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}