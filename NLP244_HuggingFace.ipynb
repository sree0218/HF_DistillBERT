{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQnPZNDnSlNP",
        "outputId": "6a60f665-3375-4302-8b58-de1567669e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "import random"
      ],
      "metadata": {
        "id": "1mYEYbYLyytJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading Hugging Face Transformers"
      ],
      "metadata": {
        "id": "KibLiEQy4eJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install datasets\n",
        "!pip install transformers torch\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "DSwhFSr9V1L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing required libs"
      ],
      "metadata": {
        "id": "5FafQWqN4loX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import list_datasets, load_dataset, DatasetDict\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Union, Callable, Any\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import torch"
      ],
      "metadata": {
        "id": "JH5P0s_PTdB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5URCIas6ivFH",
        "outputId": "8f30f7ba-561b-48ae-e1c2-22aa4659c7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flag1:downloading the datasets from HuggingFace"
      ],
      "metadata": {
        "id": "F4RIh4g4cQc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading the dataset\n",
        "datasets = list_datasets()\n",
        "snli_dataset = list_datasets(with_details=True)[datasets.index('snli')]\n",
        "# pprint(snli_dataset.__dict__) \n",
        "snli_ds = load_dataset('snli')  #loading snli dataset\n",
        "print(snli_ds)"
      ],
      "metadata": {
        "id": "P6ohpgExXwHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ðŸ‘‰ Dataset len(dataset): {len(snli_ds)}\")\n",
        "print(\"\\nðŸ‘‰ First item 'dataset[0]':\")\n",
        "pprint(snli_ds['train'][0])"
      ],
      "metadata": {
        "id": "3dsgboS3d3J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = random.choices(snli_ds['train'], k=20000)\n",
        "val_data = random.choices(snli_ds['validation'], k=4000)\n",
        "test_data = random.choices(snli_ds['test'], k=4000)\n",
        "# train_data_hyp = [ele['hypothesis'] for ele in train_data]\n",
        "# train_data_pre = [ele['premise'] for ele in train_data]\n",
        "# val_data_hyp = [ele['hypothesis'] for ele in val_data]\n",
        "# val_data_pre = [ele['premise'] for ele in val_data]\n",
        "# test_data_hyp = [ele['hypothesis'] for ele in test_data]\n",
        "# test_data_pre = [ele['premise'] for ele in test_data]"
      ],
      "metadata": {
        "id": "rYtXd_tCiNP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(val_data), len(val_data)"
      ],
      "metadata": {
        "id": "PCvHkO3mMcG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flag:2 English -> French Translations "
      ],
      "metadata": {
        "id": "mof_xogS4sAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "fBY4QQ4RVuaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class SNLI_dataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    task_prefix = \"translate English to French: \"\n",
        "    premise = task_prefix+ self.data[idx]['premise']\n",
        "    hypothesis = task_prefix+self.data[idx]['hypothesis']\n",
        "    label = self.data[idx]['label']\n",
        "    premise_ids = self.tokenizer.encode(premise, max_length=300, padding='max_length', truncation=True)\n",
        "    hypothesis_ids = self.tokenizer.encode(hypothesis, max_length=300, padding='max_length', truncation=True)\n",
        "    label_id = torch.tensor(label, device=device)\n",
        "    return torch.tensor(hypothesis_ids, device=device),torch.tensor(premise_ids, device=device),label\n",
        "\n",
        "train_dataset = SNLI_dataset(train_data, tokenizer)\n",
        "val_dataset = SNLI_dataset(val_data, tokenizer)\n",
        "test_dataset = SNLI_dataset(test_data, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "NRSWFbidhelk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "08UrrK52qD0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translation_French_English(loader):\n",
        "  english_text_hyp=[]\n",
        "  english_text_pre=[]\n",
        "  english_text_lables=[]\n",
        "  for hyp, pre,labels in loader:\n",
        "    hyp_tokens = model.generate(input_ids=hyp,max_length=300,num_beams=4,early_stopping=True)\n",
        "    for i in range(0,len(hyp_tokens)):\n",
        "      english_text_hyp.append(tokenizer.decode(hyp_tokens[i], skip_special_tokens=True))\n",
        "\n",
        "    pre_tokens = model.generate(input_ids=pre,max_length=300,num_beams=4,early_stopping=True)\n",
        "    for i in range(0,len(pre_tokens)):\n",
        "      english_text_pre.append(tokenizer.decode(pre_tokens[i], skip_special_tokens=True))\n",
        "\n",
        "    for i in range(0,len(labels)):\n",
        "      english_text_lables.append(labels[i].item())\n",
        "    \n",
        "\n",
        "  return english_text_hyp, english_text_pre, english_text_lables"
      ],
      "metadata": {
        "id": "nZ60-vF73bzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_hyp, train_pre, train_label = translation_French_English(train_dataloader)\n",
        "print(\"train translated\")\n",
        "val_hyp, val_pre, val_label = translation_French_English(val_dataloader)\n",
        "print(\"val translated\")\n",
        "test_hyp, test_pre, test_label = translation_French_English(test_dataloader)\n",
        "print(\"test translated\")\n",
        "train_hyp, train_pre, train_label"
      ],
      "metadata": {
        "id": "reHJ5hKF42TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting the translations to .csv files\n"
      ],
      "metadata": {
        "id": "FI1m40N046WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_train=\"/content/drive/MyDrive/snli/train.csv\"\n",
        "filepath_val=\"/content/drive/MyDrive/snli/val.csv\"\n",
        "filepath_test=\"/content/drive/MyDrive/snli/test.csv\""
      ],
      "metadata": {
        "id": "n7fOTy7eBTq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.DataFrame(list(zip(train_hyp, train_pre,train_label)),columns =['hypothesis', 'premise', 'label'])\n",
        "val_df = pd.DataFrame(list(zip(val_hyp, val_pre,train_label)),columns =['hypothesis', 'premise', 'label'])\n",
        "test_df = pd.DataFrame(list(zip(test_hyp, val_pre,train_label)),columns =['hypothesis', 'premise', 'label'])"
      ],
      "metadata": {
        "id": "cBgohGFZN_VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(filepath_train,index=False)\n",
        "val_df.to_csv(filepath_val,index=False)\n",
        "test_df.to_csv(filepath_test,index=False)"
      ],
      "metadata": {
        "id": "hQmNw1isSNxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = pd.read_csv(filepath_train)\n",
        "val_ds = pd.read_csv(filepath_val)\n",
        "test_ds = pd.read_csv(filepath_test)"
      ],
      "metadata": {
        "id": "BYxrJSW14cL8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}